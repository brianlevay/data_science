{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SECTION 1: GETTING THE API DATA\n",
    "\n",
    "This section is a one-time-use block of code meant to download the relevant data from the Entrez PubMed database and write it to a file. All future references to the data will come from the file itself, to save network bandwidth.\n",
    "\n",
    "**Storage Format for Each Publication:**\n",
    "\n",
    "PUBMED_ID  \n",
    "Number  \n",
    "ABSTRACT  \n",
    "Abstract text  \n",
    "MESH  \n",
    "List of MeSH terms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "This section imports the necessary modules to implement the API calls. The main package used for this section is the Entrez submodule from Biopython."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import time\n",
    "from Bio import Entrez\n",
    "Entrez.email = \"brianjlevay@gmail.com\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the Function\n",
    "\n",
    "This section defines a generic function for getting PubMed records and writing the relevant data to a file. The function accepts search terms and a filename as arguments. The flow is as follows:\n",
    "\n",
    "1. Open a file for writing\n",
    "2. Perform an initial pubmed search (eSearch), and store the results on the server (usehistory)\n",
    "3. Iteratively perform API calls (eFetch) to get the results, 10000 at a time\n",
    "4. Write the relevant parts of each result (number, abstract, and mesh terms) to a file\n",
    "5. Close the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_api_data(terms, filename):\n",
    "    f = open(filename + '.txt', 'w')\n",
    "    \n",
    "    handle = Entrez.esearch(db='pubmed', term=terms, usehistory='y')\n",
    "    search = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    count = int(search['Count'])\n",
    "    query = search['QueryKey']\n",
    "    web = search['WebEnv']\n",
    "    print(\"{} records found through eSearch.\".format(count))\n",
    "    \n",
    "    max_ret = 10000\n",
    "    steps_tot = math.ceil(count / max_ret)\n",
    "    steps = [x*max_ret for x in range(0,steps_tot)]\n",
    "    total_records = 0\n",
    "    \n",
    "    for step in steps:\n",
    "        time.sleep(30)\n",
    "        handle = Entrez.efetch(db='pubmed', query_key=query, WebEnv=web, retmode='xml', retstart=step, retmax=max_ret)\n",
    "        fetch = Entrez.read(handle)\n",
    "        handle.close()\n",
    "        print(\"Step {}: API batch returned.\".format(step))\n",
    "    \n",
    "        for entry in fetch:\n",
    "            total_records += 1\n",
    "            f.write('PUBMED_ID\\n')\n",
    "            f.write(entry['PubmedData']['ArticleIdList'][0] + '\\n')\n",
    "            f.write('ABSTRACT\\n')\n",
    "            abstract = ''\n",
    "            try:\n",
    "                abstract = entry['MedlineCitation']['Article']['Abstract']['AbstractText'][0].encode('cp1252', 'replace').decode('cp1252')\n",
    "            except:\n",
    "                abstract = 'Not available or not able to be printed.'\n",
    "            f.write(abstract + '\\n')\n",
    "            f.write('MESH\\n')\n",
    "            mesh_str = ''\n",
    "            for mesh in entry['MedlineCitation']['MeshHeadingList']:\n",
    "                mesh_str += mesh['DescriptorName'] + '; '\n",
    "            f.write(mesh_str[0:len(mesh_str)-2] + '\\n')    \n",
    "            f.write('\\n')\n",
    "    \n",
    "    f.close()\n",
    "    print(\"{} records retrieved via eFetch and written to the file.\".format(total_records))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Function to Gather the Data\n",
    "\n",
    "This block of code uses the generalized function defined above to retrieve the data from PubMed. This function only needs to be run once, and afterwards, the data will be stored in a local file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Need to specify obesity as the major MeSH descriptor (MajorTopicYN=\"Y\") [majr] vs [majr:noexp]\n",
    "\n",
    "obesity_terms = 'obesity[majr] 2000:2012[pdat]'\n",
    "\n",
    "# Only need to run this function once to get all of the relevant API data\n",
    "\n",
    "#get_api_data(obesity_terms, 'obesity_pubmed')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "# SECTION 2: CATEGORIZING MESH TERMS\n",
    "\n",
    "This section uses the MeSH definitions file (desc2015.xml) to create a list of terms that refer to \"Disease or Syndrome\". The list will then be used to filter the MeSH terms in the publication results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting Up the Environment\n",
    "\n",
    "This section imports BeautifulSoup, which is used for XML parsing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining the XML Parsing Function\n",
    "\n",
    "The source file for the MeSH terms is a large (~300 MB) xml file, and my preferred XML parser (BeautifulSoup) doesn't perform very well under such a load. So, for this exercise, I will initially split the XML file into chunks using basic string techniques, and then I will apply the parser to each fragment. This is slow, but the memory footprint is smaller and it runs without crashing. I know there are better XML libraries, but this is what I've got for now.\n",
    "\n",
    "This function opens the definitions file, extracts only the descriptors that match a SemanticTypeName specified as an argument, and writes those terms to another file. It's important to note that this function only considers semantic types listed under the preferred concept!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def get_descriptors(semantic_type, filename):\n",
    "    f = open('desc2015.xml', 'r')\n",
    "    xml_contents = f.read()\n",
    "    f.close()\n",
    "\n",
    "    f = open(filename + '.txt', 'w')\n",
    "\n",
    "    header_content = '<?xml version=\"1.0\"?>\\n' + '<!DOCTYPE DescriptorRecordSet SYSTEM \"desc2015.dtd\">\\n' + \\\n",
    "        '<DescriptorRecordSet LanguageCode = \"eng\">\\n' + '<DescriptorRecord DescriptorClass = \"1\">\\n'\n",
    "    xml_contents = xml_contents.replace(header_content, '')\n",
    "    descriptors = xml_contents.split('</DescriptorRecord>\\n<DescriptorRecord DescriptorClass = \"1\">')\n",
    "\n",
    "    for descriptor in descriptors:\n",
    "        descriptor = '<DescriptorRecord DescriptorClass = \"1\">\\n' + descriptor + '</DescriptorRecord>'\n",
    "        desc_soup = BeautifulSoup(descriptor, \"xml\")\n",
    "        name = desc_soup.DescriptorName.String.get_text()\n",
    "        semantic_tags = desc_soup.ConceptList.find('Concept', PreferredConceptYN='Y').find_all('SemanticTypeName')\n",
    "        semantic_types = set()\n",
    "        for tag in semantic_tags:\n",
    "            semantic_types.add(tag.get_text())\n",
    "        if semantic_type in semantic_types: \n",
    "            f.write(name + \"\\n\")\n",
    "    \n",
    "    f.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running the Function to Output the Descriptors\n",
    "\n",
    "This block of code runs the function defined above to generate a file with a list of applicable descriptors. You only need to run this once, and all future data access will come from the newly created file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Only need to run this function once to get all of the relevant terms\n",
    "\n",
    "#get_descriptors('Disease or Syndrome', 'mesh_disease_syndrome_terms')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
